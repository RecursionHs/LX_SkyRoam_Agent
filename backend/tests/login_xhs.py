#!/usr/bin/env python3
"""
å°çº¢ä¹¦ç™»å½•æ¼”ç¤ºè„šæœ¬
æµ‹è¯•PlaywrightçœŸå®çˆ¬è™«çš„ç™»å½•å’Œæœç´¢åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from app.platforms.xhs.real_crawler import XiaoHongShuLoginCrawler


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å°çº¢ä¹¦çœŸå®æ•°æ®çˆ¬è™« - ç™»å½•æ¼”ç¤º")
    logger.info("=" * 50)
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = XiaoHongShuLoginCrawler()
    
    try:
        logger.info("æ­¥éª¤1: å¼€å§‹ç™»å½•æµç¨‹")
        
        # å¯åŠ¨çˆ¬è™«ï¼ˆä¼šè‡ªåŠ¨å°è¯•åŠ è½½cookiesï¼‰
        await crawler.start()
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        if not crawler.is_logged_in:
            logger.info("éœ€è¦æ‰‹åŠ¨ç™»å½•...")
            success = await crawler.login()
            if not success:
                logger.error("âŒ ç™»å½•å¤±è´¥")
                return
        
        logger.info("\næ­¥éª¤2: æµ‹è¯•æœç´¢åŠŸèƒ½")
        
        # æµ‹è¯•æœç´¢å…³é”®è¯
        test_keywords = ["åŒ—äº¬æ—…æ¸¸", "ä¸Šæµ·ç¾é£Ÿ", "æˆéƒ½æ™¯ç‚¹"]
        
        for keyword in test_keywords:
            logger.info(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
            try:
                notes = await crawler.search_notes(keyword, limit=5)
                
                if notes:
                    logger.info(f"âœ… æˆåŠŸè·å– {len(notes)} æ¡ç¬”è®°æ•°æ®")
                    
                    # æ˜¾ç¤ºå‰3æ¡ç¬”è®°çš„åŸºæœ¬ä¿¡æ¯
                    for i, note in enumerate(notes[:3], 1):
                        logger.info(f"  {i}. {note.get('title', 'æ— æ ‡é¢˜')[:30]}...")
                        logger.info(f"     ğŸ‘ {note.get('like_count', 0)} | ğŸ’¬ {note.get('comment_count', 0)}")
                else:
                    logger.warning(f"âš ï¸ æœªè·å–åˆ° {keyword} çš„ç¬”è®°æ•°æ®")
                    
            except Exception as e:
                logger.error(f"âŒ æœç´¢ {keyword} å¤±è´¥: {e}")
            
            # ç­‰å¾…ä¸€ä¸‹é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)
        
        logger.info("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        
        # æ˜¾ç¤ºcookieä¿¡æ¯
        cookie_info = crawler.crawler.get_cookie_info()
        if cookie_info["exists"]:
            logger.info(f"ğŸ“ Cookieå·²ä¿å­˜ï¼Œæœ‰æ•ˆæœŸè¿˜æœ‰ {7 - float(cookie_info['age_days']):.1f} å¤©")
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        
    finally:
        # å…³é—­çˆ¬è™«
        await crawler.close()


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(sys.stdout, level="INFO", format="{time:HH:mm:ss} | {level:<8} | {message}")
    
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("ğŸ‘‹ ç”¨æˆ·å–æ¶ˆæ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")