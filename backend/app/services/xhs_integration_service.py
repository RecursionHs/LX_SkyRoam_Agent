"""
å°çº¢ä¹¦æ•°æ®é›†æˆæœåŠ¡
è´Ÿè´£å°†å°çº¢ä¹¦ç¬”è®°æ•°æ®é›†æˆåˆ°æ—…è¡Œæ”»ç•¥ç³»ç»Ÿä¸­
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from loguru import logger
import traceback
import re
from dataclasses import dataclass

# æš‚æ—¶æ³¨é‡Šå¤æ‚çš„å°çº¢ä¹¦çˆ¬è™«ä¾èµ–ï¼Œé¿å…é…ç½®é—®é¢˜
# from app.platforms.xhs.client import XiaoHongShuClient
# from app.platforms.xhs.core import XiaoHongShuCrawler
# from app.platforms.xhs.field import SearchSortType, SearchNoteType
from app.core.redis import get_cache, set_cache, cache_key


@dataclass
class XHSNoteData:
    """å°çº¢ä¹¦ç¬”è®°æ•°æ®ç»“æ„"""
    note_id: str
    title: str
    desc: str
    type: str
    user_info: Dict[str, Any]
    img_urls: List[str]
    video_url: str
    tag_list: List[str]
    collected_count: int
    comment_count: int
    liked_count: int
    share_count: int
    publish_time: datetime
    location: Optional[str] = None
    relevance_score: float = 0.0


class XHSIntegrationService:
    """å°çº¢ä¹¦æ•°æ®é›†æˆæœåŠ¡"""
    
    def __init__(self):
        self.xhs_crawler = None
        self.max_notes_per_destination = 50  # æ¯ä¸ªç›®çš„åœ°æœ€å¤šè·å–çš„ç¬”è®°æ•°é‡
        self.cache_ttl = 3600 * 6  # ç¼“å­˜6å°æ—¶
        
    async def get_destination_notes(
        self, 
        destination: str, 
        keywords: Optional[List[str]] = None,
        sort_type: str = "most_popular"  # æš‚æ—¶ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹
    ) -> List[XHSNoteData]:
        """
        è·å–ç›®çš„åœ°ç›¸å…³çš„å°çº¢ä¹¦ç¬”è®°
        
        Args:
            destination: ç›®çš„åœ°åç§°
            keywords: é¢å¤–çš„å…³é”®è¯åˆ—è¡¨
            sort_type: æ’åºæ–¹å¼
            
        Returns:
            List[XHSNoteData]: ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢å…³é”®è¯
            search_keywords = self._build_search_keywords(destination, keywords)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key_str = cache_key("xhs_notes", destination, str(sort_type))
            cached_data = await get_cache(cache_key_str)
            if cached_data:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å°çº¢ä¹¦æ•°æ®: {destination}")
                return [XHSNoteData(**note) for note in cached_data]
            
            logger.info(f"å¼€å§‹è·å–å°çº¢ä¹¦ç¬”è®°: {destination}")
            
            # åˆå§‹åŒ–çˆ¬è™«ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.xhs_crawler:
                await self._init_crawler()
            
            all_notes = []
            
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
            for keyword in search_keywords:
                try:
                    notes = await self._search_notes_by_keyword(
                        keyword=keyword,
                        sort_type=sort_type,
                        max_count=20  # æ¯ä¸ªå…³é”®è¯æœ€å¤š20æ¡
                    )
                    all_notes.extend(notes)
                    
                    # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    continue
            
            # å»é‡å’Œæ’åº
            unique_notes = self._deduplicate_and_rank_notes(all_notes, destination)
            
            # é™åˆ¶æ•°é‡
            final_notes = unique_notes[:self.max_notes_per_destination]
            
            # ç¼“å­˜ç»“æœ
            cache_data = [note.__dict__ for note in final_notes]
            await set_cache(cache_key_str, cache_data, ttl=self.cache_ttl)
            
            logger.info(f"æˆåŠŸè·å– {len(final_notes)} æ¡å°çº¢ä¹¦ç¬”è®°: {destination}")
            return final_notes
            
        except Exception as e:
            logger.error(f"è·å–å°çº¢ä¹¦ç¬”è®°å¤±è´¥: {destination}, é”™è¯¯: {e}")
            return []
    
    def _build_search_keywords(self, destination: str, extra_keywords: Optional[List[str]] = None) -> List[str]:
        """æ„å»ºæœç´¢å…³é”®è¯åˆ—è¡¨"""
        keywords = [
            destination,
            f"{destination}æ—…æ¸¸",
            f"{destination}æ”»ç•¥",
            f"{destination}æ™¯ç‚¹",
            f"{destination}ç¾é£Ÿ",
            f"{destination}ä½å®¿"
        ]
        
        if extra_keywords:
            keywords.extend([f"{destination}{kw}" for kw in extra_keywords])
        
        return keywords
    
    async def _init_crawler(self):
        """åˆå§‹åŒ–å°çº¢ä¹¦çˆ¬è™«"""
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨PlaywrightçœŸå®çˆ¬è™«
            try:
                from app.platforms.xhs.real_crawler import XiaoHongShuRealCrawler
                self.xhs_crawler = XiaoHongShuRealCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦PlaywrightçœŸå®çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"PlaywrightçœŸå®çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨æ—…æ¸¸çˆ¬è™«
            try:
                from app.platforms.xhs.travel_core import XiaoHongShuTravelCrawler
                self.xhs_crawler = XiaoHongShuTravelCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦æ—…æ¸¸çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"æ—…æ¸¸çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨åŸå§‹çˆ¬è™«
            try:
                from app.platforms.xhs.core import XiaoHongShuCrawler
                self.xhs_crawler = XiaoHongShuCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦åŸå§‹çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"åŸå§‹çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰çœŸå®çˆ¬è™«éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            logger.warning("æ‰€æœ‰çœŸå®çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
            self.use_mock_data = True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–çˆ¬è™«å¤±è´¥: {e}")
            self.use_mock_data = True
    
    async def _search_notes_by_keyword(
        self, 
        keyword: str, 
        sort_type: str,
        max_count: int = 20
    ) -> List[XHSNoteData]:
        """æ ¹æ®å…³é”®è¯æœç´¢ç¬”è®°"""
        try:
            # ä½¿ç”¨çœŸå®çˆ¬è™«æœç´¢
            logger.info(f"ä½¿ç”¨çœŸå®çˆ¬è™«æœç´¢ç¬”è®°: {keyword}")
            
            # æ£€æŸ¥çˆ¬è™«æ˜¯å¦æœ‰searchæ–¹æ³•ï¼ˆæ–°çš„æ—…æ¸¸çˆ¬è™«ï¼‰
            if hasattr(self.xhs_crawler, 'search'):
                notes_data = await self.xhs_crawler.search(keyword, max_count)
                notes = []
                for note_data in notes_data:
                    try:
                        note = self._parse_note_item(note_data)
                        if note:
                            notes.append(note)
                    except Exception as e:
                        logger.warning(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
                        continue
                return notes
            
            # æ£€æŸ¥çˆ¬è™«æ˜¯å¦æœ‰xhs_clientï¼ˆåŸå§‹çˆ¬è™«ï¼‰
            if hasattr(self.xhs_crawler, 'xhs_client') and self.xhs_crawler.xhs_client:
                # æœç´¢ç¬”è®°
                search_result = await self.xhs_crawler.xhs_client.get_note_by_keyword(
                    keyword=keyword,
                    page=1,
                    page_size=min(max_count, 20),  # å°çº¢ä¹¦APIé™åˆ¶
                    sort=sort_type,
                    note_type="all"  # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯æšä¸¾
                )
                
                if not search_result or 'data' not in search_result:
                    logger.warning(f"æœç´¢ç»“æœä¸ºç©º: {keyword}")
                    return self._generate_mock_notes(keyword, max_count)
                
                notes = []
                for item in search_result['data']['items'][:max_count]:
                    try:
                        note_data = self._parse_note_item(item)
                        if note_data:
                            notes.append(note_data)
                    except Exception as e:
                        logger.warning(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
                        continue
                
                return notes
            
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
            logger.warning("çˆ¬è™«æ²¡æœ‰å¯ç”¨çš„æœç´¢æ–¹æ³•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._generate_mock_notes(keyword, max_count)
            
        except Exception as e:
            logger.error(f"æœç´¢ç¬”è®°å¤±è´¥: {keyword}, é”™è¯¯: {e}")
            # å‡ºé”™æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return self._generate_mock_notes(keyword, max_count)
    
    def _parse_note_item(self, item: Dict[str, Any]) -> Optional[XHSNoteData]:
        """è§£æç¬”è®°æ•°æ®é¡¹"""
        try:
            note_card = item.get('note_card', {})
            if not note_card:
                return None
            
            # æå–åŸºæœ¬ä¿¡æ¯
            note_id = note_card.get('note_id', '')
            title = note_card.get('display_title', '')
            desc = note_card.get('desc', '')
            note_type = note_card.get('type', 'normal')
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            user_info = note_card.get('user', {})
            
            # æå–åª’ä½“ä¿¡æ¯
            img_urls = []
            video_url = ''
            
            if 'image_list' in note_card:
                img_urls = [img.get('url_default', '') for img in note_card['image_list']]
            
            if 'video' in note_card:
                video_url = note_card['video'].get('url_default', '')
            
            # æå–æ ‡ç­¾
            tag_list = []
            if 'tag_list' in note_card:
                tag_list = [tag.get('name', '') for tag in note_card['tag_list']]
            
            # æå–ç»Ÿè®¡æ•°æ®
            interact_info = note_card.get('interact_info', {})
            collected_count = int(interact_info.get('collected_count', 0))
            comment_count = int(interact_info.get('comment_count', 0))
            liked_count = int(interact_info.get('liked_count', 0))
            share_count = int(interact_info.get('share_count', 0))
            
            # æå–æ—¶é—´
            time_str = note_card.get('time', '')
            publish_time = self._parse_time(time_str)
            
            # æå–ä½ç½®ä¿¡æ¯
            location = note_card.get('location', {}).get('name', '')
            
            return XHSNoteData(
                note_id=note_id,
                title=title,
                desc=desc,
                type=note_type,
                user_info=user_info,
                img_urls=img_urls,
                video_url=video_url,
                tag_list=tag_list,
                collected_count=collected_count,
                comment_count=comment_count,
                liked_count=liked_count,
                share_count=share_count,
                publish_time=publish_time,
                location=location
            )
            
        except Exception as e:
            logger.error(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _parse_time(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            if not time_str:
                return datetime.now()
            
            # å¤„ç†ç›¸å¯¹æ—¶é—´æ ¼å¼
            if 'åˆ†é’Ÿå‰' in time_str:
                minutes = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(minutes=minutes)
            elif 'å°æ—¶å‰' in time_str:
                hours = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(hours=hours)
            elif 'å¤©å‰' in time_str:
                days = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(days=days)
            else:
                # å°è¯•è§£æå…·ä½“æ—¥æœŸ
                return datetime.strptime(time_str, '%Y-%m-%d')
        except:
            return datetime.now()
    
    def _deduplicate_and_rank_notes(self, notes: List[XHSNoteData], destination: str) -> List[XHSNoteData]:
        """å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åºç¬”è®°"""
        # å»é‡ï¼ˆåŸºäºnote_idï¼‰
        unique_notes = {}
        for note in notes:
            if note.note_id not in unique_notes:
                unique_notes[note.note_id] = note
        
        notes_list = list(unique_notes.values())
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        for note in notes_list:
            note.relevance_score = self._calculate_relevance_score(note, destination)
        
        # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        notes_list.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return notes_list
    
    def _calculate_relevance_score(self, note: XHSNoteData, destination: str) -> float:
        """è®¡ç®—ç¬”è®°ä¸ç›®çš„åœ°çš„ç›¸å…³æ€§å¾—åˆ†"""
        score = 0.0
        
        # æ ‡é¢˜ç›¸å…³æ€§ (æƒé‡: 0.3)
        if destination in note.title:
            score += 0.3
        
        # æè¿°ç›¸å…³æ€§ (æƒé‡: 0.2)
        if destination in note.desc:
            score += 0.2
        
        # ä½ç½®ç›¸å…³æ€§ (æƒé‡: 0.2)
        if note.location and destination in note.location:
            score += 0.2
        
        # æ ‡ç­¾ç›¸å…³æ€§ (æƒé‡: 0.1)
        travel_tags = ['æ—…æ¸¸', 'æ”»ç•¥', 'æ™¯ç‚¹', 'ç¾é£Ÿ', 'ä½å®¿', 'æ‰“å¡']
        for tag in note.tag_list:
            if any(travel_tag in tag for travel_tag in travel_tags):
                score += 0.1
                break
        
        # äº’åŠ¨æ•°æ®æƒé‡ (æƒé‡: 0.2)
        # å½’ä¸€åŒ–å¤„ç†ï¼Œé¿å…æ•°å€¼è¿‡å¤§
        interaction_score = (
            min(note.liked_count / 1000, 1.0) * 0.1 +
            min(note.collected_count / 500, 1.0) * 0.1
        )
        score += interaction_score
        
        return min(score, 1.0)  # é™åˆ¶æœ€å¤§å¾—åˆ†ä¸º1.0
    
    def format_notes_for_llm(self, notes: List[XHSNoteData], destination: str) -> str:
        """å°†ç¬”è®°æ•°æ®æ ¼å¼åŒ–ä¸ºé€‚åˆLLMå¤„ç†çš„æ–‡æœ¬"""
        if not notes:
            return f"æœªæ‰¾åˆ°å…³äº{destination}çš„å°çº¢ä¹¦ç¬”è®°æ•°æ®ã€‚"
        
        formatted_text = f"=== {destination} å°çº¢ä¹¦çœŸå®ç”¨æˆ·åˆ†äº« ===\n\n"
        
        for i, note in enumerate(notes[:10], 1):  # åªå–å‰10æ¡æœ€ç›¸å…³çš„
            formatted_text += f"ã€ç¬”è®° {i}ã€‘\n"
            formatted_text += f"æ ‡é¢˜: {note.title}\n"
            formatted_text += f"å†…å®¹: {note.desc[:200]}{'...' if len(note.desc) > 200 else ''}\n"
            
            if note.location:
                formatted_text += f"ä½ç½®: {note.location}\n"
            
            if note.tag_list:
                formatted_text += f"æ ‡ç­¾: {', '.join(note.tag_list[:5])}\n"
            
            formatted_text += f"äº’åŠ¨æ•°æ®: ğŸ‘{note.liked_count} ğŸ’¾{note.collected_count} ğŸ’¬{note.comment_count}\n"
            formatted_text += f"å‘å¸ƒæ—¶é—´: {note.publish_time.strftime('%Y-%m-%d')}\n"
            formatted_text += f"ç›¸å…³æ€§å¾—åˆ†: {note.relevance_score:.2f}\n\n"
        
        formatted_text += f"ä»¥ä¸Šæ˜¯æ¥è‡ªå°çº¢ä¹¦çš„çœŸå®ç”¨æˆ·åˆ†äº«ï¼ŒåŒ…å«äº†{len(notes)}æ¡ç›¸å…³ç¬”è®°ã€‚"
        formatted_text += "è¿™äº›å†…å®¹åæ˜ äº†çœŸå®ç”¨æˆ·çš„ä½“éªŒå’Œå»ºè®®ï¼Œè¯·åœ¨ç”Ÿæˆæ”»ç•¥æ—¶é‡ç‚¹å‚è€ƒã€‚\n"
        
        return formatted_text
    
    def _generate_mock_notes(self, destination: str, keywords: Optional[List[str]] = None, max_notes: int = 10) -> List[XHSNoteData]:
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„å°çº¢ä¹¦ç¬”è®°æ•°æ®ç”¨äºæµ‹è¯•"""
        from datetime import datetime, timedelta
        import random
        
        # æ ¹æ®ç›®çš„åœ°ç±»å‹ç”Ÿæˆä¸åŒçš„ç¬”è®°æ¨¡æ¿
        city_templates = {
            "åŒ—äº¬": [
                {"title": "åŒ—äº¬æ•…å®«æ·±åº¦æ¸¸ï½œé¿å¼€äººç¾¤çš„æœ€ä½³è·¯çº¿", "desc": "æ•…å®«å¤ªå¤§äº†ï¼åˆ†äº«ä¸€æ¡é¿å¼€äººç¾¤çš„æ¸¸è§ˆè·¯çº¿ï¼Œè¿˜æœ‰æ‹ç…§æœºä½æ¨èï¼Œè®©ä½ è½»æ¾é€›å®Œç´«ç¦åŸï½", "tags": ["æ•…å®«", "åŒ—äº¬", "é¿å‘", "æ‹ç…§"]},
                {"title": "åŒ—äº¬èƒ¡åŒæ¢ç§˜ï½œæœ€æœ‰å‘³é“çš„è€åŒ—äº¬ç”Ÿæ´»", "desc": "èµ°è¿›å—é”£é¼“å··ã€ä»€åˆ¹æµ·èƒ¡åŒï¼Œæ„Ÿå—æœ€åœ°é“çš„è€åŒ—äº¬æ–‡åŒ–ï¼Œè¿˜æœ‰éšè—çš„å°åº—æ¨èï¼", "tags": ["èƒ¡åŒ", "åŒ—äº¬", "æ–‡åŒ–", "è€åŒ—äº¬"]},
                {"title": "åŒ—äº¬çƒ¤é¸­å“ªå®¶å¼ºï¼Ÿå…¨èšå¾·vsä¾¿å®œåŠå®æµ‹", "desc": "ä½œä¸ºåŒ—äº¬åœŸè‘—ï¼Œå®æµ‹äº†5å®¶çƒ¤é¸­åº—ï¼Œå‘Šè¯‰ä½ å“ªå®¶æœ€æ­£å®—æœ€å¥½åƒï¼Œé¿å…è¸©é›·ï¼", "tags": ["çƒ¤é¸­", "åŒ—äº¬", "ç¾é£Ÿ", "æµ‹è¯„"]},
                {"title": "åŒ—äº¬åœ°é“å‡ºè¡Œæ”»ç•¥ï½œæ–°æ‰‹å¿…çœ‹", "desc": "åŒ—äº¬åœ°é“çº¿è·¯å¤æ‚ï¼Ÿè¿™ç¯‡æ”»ç•¥æ•™ä½ å¦‚ä½•é«˜æ•ˆæ¢ä¹˜ï¼Œè¿˜æœ‰å„ç§ä¼˜æƒ ç¥¨æ¨èï¼", "tags": ["åœ°é“", "åŒ—äº¬", "äº¤é€š", "æ”»ç•¥"]}
            ],
            "ä¸Šæµ·": [
                {"title": "ä¸Šæµ·å¤–æ»©æœ€ä½³è§‚æ™¯æ—¶é—´ï½œæ—¥è½å¤œæ™¯éƒ½ç»äº†", "desc": "å¤–æ»©ä»€ä¹ˆæ—¶å€™å»æœ€ç¾ï¼Ÿåˆ†äº«æœ€ä½³è§‚æ™¯æ—¶é—´å’Œæ‹ç…§è§’åº¦ï¼Œè¿˜æœ‰å‘¨è¾¹ç¾é£Ÿæ¨èï¼", "tags": ["å¤–æ»©", "ä¸Šæµ·", "å¤œæ™¯", "æ‹ç…§"]},
                {"title": "ä¸Šæµ·è¿ªå£«å°¼çœé’±æ”»ç•¥ï½œå­¦ç”Ÿå…šå¿…çœ‹", "desc": "è¿ªå£«å°¼å¤ªè´µï¼Ÿè¿™ç¯‡æ”»ç•¥æ•™ä½ å¦‚ä½•çœé’±ç©è½¬è¿ªå£«å°¼ï¼Œé—¨ç¥¨ã€é¤é¥®ã€ä½å®¿å…¨è¦†ç›–ï¼", "tags": ["è¿ªå£«å°¼", "ä¸Šæµ·", "çœé’±", "æ”»ç•¥"]},
                {"title": "ä¸Šæµ·å°ç¬¼åŒ…æ¢åº—ï½œå—ç¿”vsé¼æ³°ä¸°è°æ›´èƒœä¸€ç­¹", "desc": "ä¸Šæµ·å°ç¬¼åŒ…å“ªå®¶æœ€æ­£å®—ï¼Ÿå®æµ‹äº†10å®¶åº—ï¼Œä»è€å­—å·åˆ°ç½‘çº¢åº—å…¨éƒ½æœ‰ï¼", "tags": ["å°ç¬¼åŒ…", "ä¸Šæµ·", "ç¾é£Ÿ", "æ¢åº—"]},
                {"title": "ä¸Šæµ·æ³•ç§Ÿç•Œæ¼«æ­¥ï½œæœ€æ–‡è‰ºçš„è¡—é“æ¨è", "desc": "æ³•ç§Ÿç•Œçš„æ¢§æ¡å¶é»„äº†ï¼æ¨èå‡ æ¡æœ€ç¾çš„è¡—é“ï¼Œé€‚åˆæ‹ç…§å’Œæ¼«æ­¥ï½", "tags": ["æ³•ç§Ÿç•Œ", "ä¸Šæµ·", "æ–‡è‰º", "æ¼«æ­¥"]}
            ]
        }
        
        # é€šç”¨æ¨¡æ¿ï¼Œé€‚ç”¨äºæ‰€æœ‰ç›®çš„åœ°
        general_templates = [
            {"title": f"{destination}ä¸‰å¤©ä¸¤å¤œå®Œç¾æ”»ç•¥ï½œè¶…è¯¦ç»†è·¯çº¿", "desc": f"åˆšä»{destination}å›æ¥ï¼Œæ•´ç†äº†è¶…è¯¦ç»†çš„æ”»ç•¥ï¼ŒåŒ…å«å¿…å»æ™¯ç‚¹ã€ç¾é£Ÿæ¨èã€äº¤é€šæŒ‡å—ï¼", "tags": ["æ”»ç•¥", destination, "ä¸‰å¤©ä¸¤å¤œ", "å¿…çœ‹"]},
            {"title": f"{destination}ç¾é£Ÿåœ°å›¾ï½œæœ¬åœ°äººæ¨è", "desc": f"åœ¨{destination}ç”Ÿæ´»å¤šå¹´ï¼Œæ¨èå‡ å®¶æœ¬åœ°äººæ‰çŸ¥é“çš„ç¾é£Ÿåº—ï¼Œå‘³é“ç»äº†ï¼", "tags": ["ç¾é£Ÿ", destination, "æœ¬åœ°æ¨è", "æ¢åº—"]},
            {"title": f"{destination}æ‹ç…§åœ£åœ°ï½œå‡ºç‰‡ç‡100%", "desc": f"{destination}æœ€å€¼å¾—æ‰“å¡çš„æ‹ç…§åœ°ï¼Œæ¯ä¸ªéƒ½è¶…å‡ºç‰‡ï¼Œå§å¦¹ä»¬ä¸€å®šè¦å»ï¼", "tags": ["æ‹ç…§", destination, "æ‰“å¡", "åœ£åœ°"]},
            {"title": f"{destination}ä½å®¿æ¨èï½œæ€§ä»·æ¯”ä¹‹ç‹", "desc": f"æ•´ç†äº†{destination}æ€§ä»·æ¯”è¶…é«˜çš„ä½å®¿ï¼Œä»é’æ—…åˆ°äº”æ˜Ÿé…’åº—ï¼Œä½ç½®å¥½ä»·æ ¼åˆç†ï¼", "tags": ["ä½å®¿", destination, "æ€§ä»·æ¯”", "æ¨è"]},
            {"title": f"{destination}äº¤é€šæ”»ç•¥ï½œæœ€çœé’±å‡ºè¡Œæ–¹å¼", "desc": f"å»{destination}ä¸çŸ¥é“æ€ä¹ˆåè½¦ï¼Ÿè¿™ç¯‡å‘Šè¯‰ä½ æœ€çœé’±æœ€æ–¹ä¾¿çš„äº¤é€šæ–¹å¼ï¼", "tags": ["äº¤é€š", destination, "çœé’±", "å‡ºè¡Œ"]},
            {"title": f"{destination}è´­ç‰©æŒ‡å—ï½œå¿…ä¹°æ¸…å•", "desc": f"{destination}è´­ç‰©æ”»ç•¥ï¼æœ¬åœ°ç‰¹äº§ã€è´­ç‰©ä¸­å¿ƒå…¨è¦†ç›–ï¼Œè¿˜æœ‰ç ä»·æŠ€å·§ï¼", "tags": ["è´­ç‰©", destination, "ç‰¹äº§", "å¿…ä¹°"]},
            {"title": f"{destination}äº²å­æ¸¸ï½œå¸¦å¨ƒå¿…å»æ™¯ç‚¹", "desc": f"å¸¦2å²å®å®å»{destination}çš„ç»éªŒåˆ†äº«ï¼Œé€‚åˆäº²å­çš„æ™¯ç‚¹å’Œå®ç”¨tipsï¼", "tags": ["äº²å­æ¸¸", destination, "å¸¦å¨ƒ", "æ™¯ç‚¹"]},
            {"title": f"{destination}å¤œç”Ÿæ´»ï½œé…’å§å¤œå¸‚æ¨è", "desc": f"{destination}çš„å¤œæ™šåŒæ ·ç²¾å½©ï¼æ¨èçƒ­é—¹çš„é…’å§è¡—å’Œå¤œå¸‚ï¼Œä½“éªŒå¤œç”Ÿæ´»ï¼", "tags": ["å¤œç”Ÿæ´»", destination, "é…’å§", "å¤œå¸‚"]}
        ]
        
        # é€‰æ‹©æ¨¡æ¿
        if destination in city_templates:
            templates = city_templates[destination] + general_templates
        else:
            templates = general_templates
        
        # éšæœºé€‰æ‹©æ¨¡æ¿
        selected_templates = random.sample(templates, min(max_notes, len(templates)))
        
        mock_notes = []
        for i, template in enumerate(selected_templates):
            # ç”Ÿæˆæ›´çœŸå®çš„äº’åŠ¨æ•°æ®
            base_popularity = random.uniform(0.5, 1.0)  # åŸºç¡€çƒ­åº¦
            liked_count = int(random.uniform(50, 8000) * base_popularity)
            collected_count = int(liked_count * random.uniform(0.1, 0.3))  # æ”¶è—é€šå¸¸æ˜¯ç‚¹èµçš„10-30%
            comment_count = int(liked_count * random.uniform(0.02, 0.1))   # è¯„è®ºé€šå¸¸æ˜¯ç‚¹èµçš„2-10%
            share_count = int(liked_count * random.uniform(0.01, 0.05))    # åˆ†äº«é€šå¸¸æ˜¯ç‚¹èµçš„1-5%
            
            # éšæœºç”Ÿæˆå‘å¸ƒæ—¶é—´ï¼ˆæœ€è¿‘60å¤©å†…ï¼Œä½†æ›´å€¾å‘äºæœ€è¿‘çš„ï¼‰
            days_ago = random.choices(
                range(1, 61), 
                weights=[60-i for i in range(60)],  # è¶Šè¿‘çš„æ—¥æœŸæƒé‡è¶Šé«˜
                k=1
            )[0]
            publish_time = datetime.now() - timedelta(days=days_ago)
            
            # æ›´ä¸°å¯Œçš„ç”¨æˆ·ä¿¡æ¯
            user_profiles = [
                {"nickname": "æ—…è¡Œè¾¾äººå°ç¾", "type": "travel_blogger"},
                {"nickname": "åƒè´§æ¢åº—ç‹", "type": "food_blogger"},
                {"nickname": "æ‘„å½±å¸ˆé˜¿å¼º", "type": "photographer"},
                {"nickname": "èƒŒåŒ…å®¢å°æ", "type": "backpacker"},
                {"nickname": "æœ¬åœ°å‘å¯¼è€å¼ ", "type": "local_guide"},
                {"nickname": "è‡ªç”±è¡Œä¸“å®¶", "type": "travel_expert"},
                {"nickname": "ç¾é£Ÿåšä¸»", "type": "food_expert"},
                {"nickname": "æ–‡è‰ºé’å¹´", "type": "culture_lover"}
            ]
            
            user_profile = random.choice(user_profiles)
            user_info = {
                "nickname": user_profile["nickname"],
                "user_id": f"user_{random.randint(100000, 999999)}",
                "avatar": f"https://example.com/avatar_{user_profile['type']}_{i}.jpg",
                "desc": f"ä¸“æ³¨{destination}æ—…æ¸¸åˆ†äº«"
            }
            
            # ç”Ÿæˆå›¾ç‰‡æ•°é‡ï¼ˆ1-9å¼ ï¼Œç¬¦åˆå°çº¢ä¹¦ç‰¹ç‚¹ï¼‰
            img_count = random.choices([1, 3, 4, 6, 9], weights=[10, 30, 25, 20, 15], k=1)[0]
            
            note = XHSNoteData(
                note_id=f"note_{destination}_{i}_{random.randint(100000, 999999)}",
                title=template["title"],
                desc=template["desc"],
                type=random.choice(["normal", "video"]) if random.random() > 0.8 else "normal",
                user_info=user_info,
                img_urls=[f"https://example.com/img_{destination}_{i}_{j}.jpg" for j in range(img_count)],
                video_url=f"https://example.com/video_{i}.mp4" if random.random() > 0.9 else "",
                tag_list=template["tags"],
                collected_count=collected_count,
                comment_count=comment_count,
                liked_count=liked_count,
                share_count=share_count,
                publish_time=publish_time,
                location=destination,
                relevance_score=random.uniform(0.75, 0.98)  # é«˜ç›¸å…³æ€§ï¼Œä½†æœ‰ä¸€å®šå˜åŒ–
            )
            mock_notes.append(note)
        
        # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        mock_notes.sort(key=lambda x: x.relevance_score, reverse=True)
        return mock_notes