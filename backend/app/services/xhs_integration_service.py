"""
å°çº¢ä¹¦æ•°æ®é›†æˆæœåŠ¡
è´Ÿè´£å°†å°çº¢ä¹¦ç¬”è®°æ•°æ®é›†æˆåˆ°æ—…è¡Œæ”»ç•¥ç³»ç»Ÿä¸­
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from loguru import logger
import traceback
import re
from dataclasses import dataclass

# æš‚æ—¶æ³¨é‡Šå¤æ‚çš„å°çº¢ä¹¦çˆ¬è™«ä¾èµ–ï¼Œé¿å…é…ç½®é—®é¢˜
# from app.platforms.xhs.client import XiaoHongShuClient
# from app.platforms.xhs.core import XiaoHongShuCrawler
# from app.platforms.xhs.field import SearchSortType, SearchNoteType
from app.core.redis import get_cache, set_cache, cache_key


@dataclass
class XHSNoteData:
    """å°çº¢ä¹¦ç¬”è®°æ•°æ®ç»“æ„"""
    note_id: str
    title: str
    desc: str
    type: str
    user_info: Dict[str, Any]
    img_urls: List[str]
    video_url: str
    tag_list: List[str]
    collected_count: int
    comment_count: int
    liked_count: int
    share_count: int
    publish_time: datetime
    location: Optional[str] = None
    relevance_score: float = 0.0


class XHSIntegrationService:
    """å°çº¢ä¹¦æ•°æ®é›†æˆæœåŠ¡"""
    
    def __init__(self):
        self.xhs_crawler = None
        self.max_notes_per_destination = 50  # æ¯ä¸ªç›®çš„åœ°æœ€å¤šè·å–çš„ç¬”è®°æ•°é‡
        self.cache_ttl = 3600 * 6  # ç¼“å­˜6å°æ—¶
        
    async def get_destination_notes(
        self, 
        destination: str, 
        keywords: Optional[List[str]] = None,
        sort_type: str = "most_popular"  # æš‚æ—¶ä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹
    ) -> List[XHSNoteData]:
        """
        è·å–ç›®çš„åœ°ç›¸å…³çš„å°çº¢ä¹¦ç¬”è®°
        
        Args:
            destination: ç›®çš„åœ°åç§°
            keywords: é¢å¤–çš„å…³é”®è¯åˆ—è¡¨
            sort_type: æ’åºæ–¹å¼
            
        Returns:
            List[XHSNoteData]: ç¬”è®°æ•°æ®åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢å…³é”®è¯
            search_keywords = self._build_search_keywords(destination, keywords)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key_str = cache_key("xhs_notes", destination, str(sort_type))
            cached_data = await get_cache(cache_key_str)
            if cached_data:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å°çº¢ä¹¦æ•°æ®: {destination}")
                return [XHSNoteData(**note) for note in cached_data]
            
            logger.info(f"å¼€å§‹è·å–å°çº¢ä¹¦ç¬”è®°: {destination}")
            
            # åˆå§‹åŒ–çˆ¬è™«ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.xhs_crawler:
                await self._init_crawler()
            
            all_notes = []
            
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢
            for keyword in search_keywords:
                try:
                    notes = await self._search_notes_by_keyword(
                        keyword=keyword,
                        sort_type=sort_type,
                        max_count=20  # æ¯ä¸ªå…³é”®è¯æœ€å¤š20æ¡
                    )
                    all_notes.extend(notes)
                    
                    # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.warning(f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}")
                    continue
            
            # å»é‡å’Œæ’åº
            unique_notes = self._deduplicate_and_rank_notes(all_notes, destination)
            
            # é™åˆ¶æ•°é‡
            final_notes = unique_notes[:self.max_notes_per_destination]
            
            # ç¼“å­˜ç»“æœ
            cache_data = [note.__dict__ for note in final_notes]
            await set_cache(cache_key_str, cache_data, ttl=self.cache_ttl)
            
            logger.info(f"æˆåŠŸè·å– {len(final_notes)} æ¡å°çº¢ä¹¦ç¬”è®°: {destination}")
            return final_notes
            
        except Exception as e:
            logger.error(f"è·å–å°çº¢ä¹¦ç¬”è®°å¤±è´¥: {destination}, é”™è¯¯: {e}")
            return []
    
    def _build_search_keywords(self, destination: str, extra_keywords: Optional[List[str]] = None) -> List[str]:
        """æ„å»ºæœç´¢å…³é”®è¯åˆ—è¡¨"""
        keywords = [
            destination,
            f"{destination}æ—…æ¸¸",
            f"{destination}æ”»ç•¥",
            f"{destination}æ™¯ç‚¹",
            f"{destination}ç¾é£Ÿ",
            f"{destination}ä½å®¿"
        ]
        
        if extra_keywords:
            keywords.extend([f"{destination}{kw}" for kw in extra_keywords])
        
        return keywords
    
    async def _init_crawler(self):
        """åˆå§‹åŒ–å°çº¢ä¹¦çˆ¬è™«"""
        try:
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨PlaywrightçœŸå®çˆ¬è™«
            try:
                from app.platforms.xhs.real_crawler import XiaoHongShuRealCrawler
                self.xhs_crawler = XiaoHongShuRealCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦PlaywrightçœŸå®çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"PlaywrightçœŸå®çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨æ—…æ¸¸çˆ¬è™«
            try:
                from app.platforms.xhs.travel_core import XiaoHongShuTravelCrawler
                self.xhs_crawler = XiaoHongShuTravelCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦æ—…æ¸¸çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"æ—…æ¸¸çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å°è¯•ä½¿ç”¨åŸå§‹çˆ¬è™«
            try:
                from app.platforms.xhs.core import XiaoHongShuCrawler
                self.xhs_crawler = XiaoHongShuCrawler()
                await self.xhs_crawler.start()
                logger.info("âœ… å°çº¢ä¹¦åŸå§‹çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"åŸå§‹çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
            
            # å¦‚æœæ‰€æœ‰çœŸå®çˆ¬è™«éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            logger.warning("æ‰€æœ‰çœŸå®çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
            self.use_mock_data = True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–çˆ¬è™«å¤±è´¥: {e}")
            self.use_mock_data = True
    
    async def _search_notes_by_keyword(
        self, 
        keyword: str, 
        sort_type: str,
        max_count: int = 20
    ) -> List[XHSNoteData]:
        """æ ¹æ®å…³é”®è¯æœç´¢ç¬”è®°"""
        try:
            # ä½¿ç”¨çœŸå®çˆ¬è™«æœç´¢
            logger.info(f"ä½¿ç”¨çœŸå®çˆ¬è™«æœç´¢ç¬”è®°: {keyword}")
            
            # æ£€æŸ¥çˆ¬è™«æ˜¯å¦æœ‰searchæ–¹æ³•ï¼ˆæ–°çš„æ—…æ¸¸çˆ¬è™«ï¼‰
            if hasattr(self.xhs_crawler, 'search'):
                notes_data = await self.xhs_crawler.search(keyword, max_count)
                notes = []
                for note_data in notes_data:
                    try:
                        note = self._parse_note_item(note_data)
                        if note:
                            notes.append(note)
                    except Exception as e:
                        logger.warning(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
                        continue
                return notes
            
            # æ£€æŸ¥çˆ¬è™«æ˜¯å¦æœ‰xhs_clientï¼ˆåŸå§‹çˆ¬è™«ï¼‰
            if hasattr(self.xhs_crawler, 'xhs_client') and self.xhs_crawler.xhs_client:
                # æœç´¢ç¬”è®°
                search_result = await self.xhs_crawler.xhs_client.get_note_by_keyword(
                    keyword=keyword,
                    page=1,
                    page_size=min(max_count, 20),  # å°çº¢ä¹¦APIé™åˆ¶
                    sort=sort_type,
                    note_type="all"  # ä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯æšä¸¾
                )
                
                if not search_result or 'data' not in search_result:
                    logger.warning(f"æœç´¢ç»“æœä¸ºç©º: {keyword}")
                    return []
                
                notes = []
                for item in search_result['data']['items'][:max_count]:
                    try:
                        note_data = self._parse_note_item(item)
                        if note_data:
                            notes.append(note_data)
                    except Exception as e:
                        logger.warning(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
                        continue
                
                return notes
            
            # å¦‚æœéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºåˆ—è¡¨
            logger.warning("çˆ¬è™«æ²¡æœ‰å¯ç”¨çš„æœç´¢æ–¹æ³•")
            return []
            
        except Exception as e:
            logger.error(f"æœç´¢ç¬”è®°å¤±è´¥: {keyword}, é”™è¯¯: {e}")
            # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨
            return []
    
    def _parse_note_item(self, item: Dict[str, Any]) -> Optional[XHSNoteData]:
        """è§£æç¬”è®°æ•°æ®é¡¹"""
        try:
            note_card = item.get('note_card', {})
            if not note_card:
                return None
            
            # æå–åŸºæœ¬ä¿¡æ¯
            note_id = note_card.get('note_id', '')
            title = note_card.get('display_title', '')
            desc = note_card.get('desc', '')
            note_type = note_card.get('type', 'normal')
            
            # æå–ç”¨æˆ·ä¿¡æ¯
            user_info = note_card.get('user', {})
            
            # æå–åª’ä½“ä¿¡æ¯
            img_urls = []
            video_url = ''
            
            if 'image_list' in note_card:
                img_urls = [img.get('url_default', '') for img in note_card['image_list']]
            
            if 'video' in note_card:
                video_url = note_card['video'].get('url_default', '')
            
            # æå–æ ‡ç­¾
            tag_list = []
            if 'tag_list' in note_card:
                tag_list = [tag.get('name', '') for tag in note_card['tag_list']]
            
            # æå–ç»Ÿè®¡æ•°æ®
            interact_info = note_card.get('interact_info', {})
            collected_count = int(interact_info.get('collected_count', 0))
            comment_count = int(interact_info.get('comment_count', 0))
            liked_count = int(interact_info.get('liked_count', 0))
            share_count = int(interact_info.get('share_count', 0))
            
            # æå–æ—¶é—´
            time_str = note_card.get('time', '')
            publish_time = self._parse_time(time_str)
            
            # æå–ä½ç½®ä¿¡æ¯
            location = note_card.get('location', {}).get('name', '')
            
            return XHSNoteData(
                note_id=note_id,
                title=title,
                desc=desc,
                type=note_type,
                user_info=user_info,
                img_urls=img_urls,
                video_url=video_url,
                tag_list=tag_list,
                collected_count=collected_count,
                comment_count=comment_count,
                liked_count=liked_count,
                share_count=share_count,
                publish_time=publish_time,
                location=location
            )
            
        except Exception as e:
            logger.error(f"è§£æç¬”è®°æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _parse_time(self, time_str: str) -> datetime:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²"""
        try:
            if not time_str:
                return datetime.now()
            
            # å¤„ç†ç›¸å¯¹æ—¶é—´æ ¼å¼
            if 'åˆ†é’Ÿå‰' in time_str:
                minutes = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(minutes=minutes)
            elif 'å°æ—¶å‰' in time_str:
                hours = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(hours=hours)
            elif 'å¤©å‰' in time_str:
                days = int(re.findall(r'(\d+)', time_str)[0])
                return datetime.now() - timedelta(days=days)
            else:
                # å°è¯•è§£æå…·ä½“æ—¥æœŸ
                return datetime.strptime(time_str, '%Y-%m-%d')
        except:
            return datetime.now()
    
    def _deduplicate_and_rank_notes(self, notes: List[XHSNoteData], destination: str) -> List[XHSNoteData]:
        """å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åºç¬”è®°"""
        # å»é‡ï¼ˆåŸºäºnote_idï¼‰
        unique_notes = {}
        for note in notes:
            if note.note_id not in unique_notes:
                unique_notes[note.note_id] = note
        
        notes_list = list(unique_notes.values())
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        for note in notes_list:
            note.relevance_score = self._calculate_relevance_score(note, destination)
        
        # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        notes_list.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return notes_list
    
    def _calculate_relevance_score(self, note: XHSNoteData, destination: str) -> float:
        """è®¡ç®—ç¬”è®°ä¸ç›®çš„åœ°çš„ç›¸å…³æ€§å¾—åˆ†"""
        score = 0.0
        
        # æ ‡é¢˜ç›¸å…³æ€§ (æƒé‡: 0.3)
        if destination in note.title:
            score += 0.3
        
        # æè¿°ç›¸å…³æ€§ (æƒé‡: 0.2)
        if destination in note.desc:
            score += 0.2
        
        # ä½ç½®ç›¸å…³æ€§ (æƒé‡: 0.2)
        if note.location and destination in note.location:
            score += 0.2
        
        # æ ‡ç­¾ç›¸å…³æ€§ (æƒé‡: 0.1)
        travel_tags = ['æ—…æ¸¸', 'æ”»ç•¥', 'æ™¯ç‚¹', 'ç¾é£Ÿ', 'ä½å®¿', 'æ‰“å¡']
        for tag in note.tag_list:
            if any(travel_tag in tag for travel_tag in travel_tags):
                score += 0.1
                break
        
        # äº’åŠ¨æ•°æ®æƒé‡ (æƒé‡: 0.2)
        # å½’ä¸€åŒ–å¤„ç†ï¼Œé¿å…æ•°å€¼è¿‡å¤§
        interaction_score = (
            min(note.liked_count / 1000, 1.0) * 0.1 +
            min(note.collected_count / 500, 1.0) * 0.1
        )
        score += interaction_score
        
        return min(score, 1.0)  # é™åˆ¶æœ€å¤§å¾—åˆ†ä¸º1.0
    
    def format_notes_for_llm(self, notes: List[XHSNoteData], destination: str) -> str:
        """å°†ç¬”è®°æ•°æ®æ ¼å¼åŒ–ä¸ºé€‚åˆLLMå¤„ç†çš„æ–‡æœ¬"""
        if not notes:
            return f"æœªæ‰¾åˆ°å…³äº{destination}çš„å°çº¢ä¹¦ç¬”è®°æ•°æ®ã€‚"
        
        formatted_text = f"=== {destination} å°çº¢ä¹¦çœŸå®ç”¨æˆ·åˆ†äº« ===\n\n"
        
        for i, note in enumerate(notes[:10], 1):  # åªå–å‰10æ¡æœ€ç›¸å…³çš„
            formatted_text += f"ã€ç¬”è®° {i}ã€‘\n"
            formatted_text += f"æ ‡é¢˜: {note.title}\n"
            formatted_text += f"å†…å®¹: {note.desc[:300]}{'...' if len(note.desc) > 300 else ''}\n"
            
            if note.location:
                formatted_text += f"ä½ç½®: {note.location}\n"
            
            # ä¿ç•™æ ‡ç­¾ä¿¡æ¯ï¼Œå¯¹æ—…è¡Œè§„åˆ’æœ‰ç”¨
            if note.tag_list:
                formatted_text += f"æ ‡ç­¾: {', '.join(note.tag_list[:3])}\n"
            
            # ä¿ç•™ç‚¹èµé‡ï¼Œä½“ç°æ”»ç•¥è®¤å¯åº¦ï¼Œä½†ç®€åŒ–å…¶ä»–äº’åŠ¨æ•°æ®
            formatted_text += f"ç‚¹èµ: {note.liked_count}\n"
            # æ³¨é‡Šæ‰æ”¶è—å’Œè¯„è®ºæ•°ï¼Œå‡å°‘ä¿¡æ¯é‡
            # formatted_text += f"äº’åŠ¨æ•°æ®: ğŸ‘{note.liked_count} ğŸ’¾{note.collected_count} ğŸ’¬{note.comment_count}\n"
            
            # ä¿ç•™å›¾ç‰‡ä¿¡æ¯ï¼Œå¯¹æ—…è¡Œè§„åˆ’å¾ˆæœ‰ç”¨
            if note.img_urls and len(note.img_urls) > 0:
                formatted_text += f"é…å›¾æ•°é‡: {len(note.img_urls)}å¼ \n"
            
            # æ³¨é‡Šæ‰å‘å¸ƒæ—¶é—´ï¼Œå¯¹æ—…è¡Œè§„åˆ’å‚è€ƒä»·å€¼ä¸å¤§
            # formatted_text += f"å‘å¸ƒæ—¶é—´: {note.publish_time.strftime('%Y-%m-%d')}\n"
            
            # æ³¨é‡Šæ‰ç›¸å…³æ€§å¾—åˆ†ï¼Œç”¨æˆ·ä¸éœ€è¦çœ‹åˆ°è¿™ä¸ªæŠ€æœ¯æŒ‡æ ‡
            # formatted_text += f"ç›¸å…³æ€§å¾—åˆ†: {note.relevance_score:.2f}\n\n"
            formatted_text += "\n"
        
        formatted_text += f"ä»¥ä¸Šæ˜¯æ¥è‡ªå°çº¢ä¹¦çš„çœŸå®ç”¨æˆ·åˆ†äº«ï¼ŒåŒ…å«äº†{len(notes)}æ¡ç›¸å…³ç¬”è®°ã€‚"
        formatted_text += "è¿™äº›å†…å®¹åæ˜ äº†çœŸå®ç”¨æˆ·çš„ä½“éªŒå’Œå»ºè®®ï¼Œè¯·åœ¨ç”Ÿæˆæ”»ç•¥æ—¶é‡ç‚¹å‚è€ƒã€‚\n"
        
        return formatted_text